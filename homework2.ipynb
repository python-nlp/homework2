{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2\n",
    "\n",
    "The maximum score of this homework is 100+10 points. Grading is listed in this table:\n",
    "\n",
    "| Grade | Score range |\n",
    "| --- | --- |\n",
    "| 5 | 85+ |\n",
    "| 4 | 70-84 |\n",
    "| 3 | 55-69 |\n",
    "| 2 | 40-54 |\n",
    "| 1 | 0-39 |\n",
    "\n",
    "Most exercises include tests which should pass if your solution is correct.\n",
    "However successful test do not guarantee that your solution is correct.\n",
    "The homework is partially autograded using many hidden tests.\n",
    "Test cells cannot be modified and empty cells cannot be deleted.\n",
    "\n",
    "Your solution should replace placeholder lines such as:\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "You don't need to write separate functions except when the function header is predefined.\n",
    "Variable names must be derived from the public test.\n",
    "Please do not add new cells, they will be ignored by the autograder.\n",
    "\n",
    "**VERY IMPORTANT** Before submitting your solution (pushing to the git repo),\n",
    "run your notebook with `Kernel -> Restart & Run All` and make sure it\n",
    "runs without exceptions.\n",
    "\n",
    "If your code fails the public tests (the ones you see), you will automatically receive 0 points for that exercise.\n",
    "\n",
    "## Submission\n",
    "\n",
    "GitHub Classroom will accept your last pushed version before the deadline.\n",
    "You do not need to send the homework to the instructor.\n",
    "\n",
    "## Plagiarism\n",
    "\n",
    "When preparing their homework, students are reminded to pay special attention to Title 32, Sections 92-93 of Code of Studies (quoted below). Any content from external sources must be stated in the students own words AND accompanied by citations. Copying and pasting from an external source should be avoided and any text copied must be placed between quotation marks. Reports that violate these rules cannot receive a passing grade.\n",
    "\n",
    "\"**Section 92**\n",
    "\n",
    "(1) The works of another person will be used as follows: a) if a work of another person is used in whole or in part (e.g. by copying, citation, translation from another language or presentation), the source and the name of the author will be indicated if this name is included in the source or – in case of orally presented works – may be clearly identified; b) the work of another person or any part of that will be used – up to a quantity reasonably corresponding to the nature and purpose of the student work – identified as quotations.\n",
    "\n",
    "(2) Instructors are entitled to review compliance with requirements in this article with computer programmes and databases.\n",
    "\n",
    "(3) The use of works of another person and the acknowledgement of use will be governed by applicable laws and the relevant rules of the specific discipline.\n",
    "\n",
    "**Section 93**\n",
    "\n",
    "(1) If a student fails to meet rules regarding use of works of another person in whole or in part, the student work will be considered as not assessable and the student will not be allowed to obtain the credit of the concerned subject in the specific term.\n",
    "\n",
    "(2) It will be deemed a disciplinary offence if a student – in breach of the rules regarding use of works of another person – submits or presents a work of another person fully or in a significant part verbatim (word for word) or in terms of its basic concepts or the combined version of several works of another person(s) as their own work.\n",
    "\n",
    "(3) Based on subsection (1) of Section 52/A. of the Higher Education Act, compliance with the rules regarding the use of works of another person in a master thesis may be reviewed up to five years following the issue of the degree certificate. In case of violation of the above rules, section 52/A of the Higher Education Act will apply.\"\n",
    "\n",
    "(BME Code of Studies, p.50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "77cf7aa37aae12a2a4d0a2ab33465390",
     "grade": false,
     "grade_id": "cell-88356aac3c2fbd17",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "716726e0cb25f8a545065e9b7de0e4ef",
     "grade": false,
     "grade_id": "cell-f7e07a713f45fc26",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Data setup\n",
    "\n",
    "You will work with a sample of appr. 1 million words from the UMBC WebBase corpus.\n",
    "There is also a smaller toy sample of a 100 sentences used for the public test and quick debugging.\n",
    "Both files are prepackaged as a zip file in your starter repository.\n",
    "\n",
    "If you are running the code from somewhere else and the data is not there, it will be downloaded and unpacked.\n",
    "You can set the data directory by changing the value of the `HW2_DATA_DIR` environmental variable.\n",
    "\n",
    "If you just cloned your starter repository in the recommended way, you should see the data extracted on the first run and no output on later runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d853a6de28a5fb54d07dbc1cdae0a562",
     "grade": false,
     "grade_id": "cell-88356aac3c2fbd18",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# setting up data directory (used by the instructor)\n",
    "\n",
    "data_dir = os.getenv(\"HW2_DATA_DIR\")\n",
    "if data_dir is None:\n",
    "    data_dir = \"\"\n",
    "    \n",
    "data_zip_path = os.path.join(data_dir, \"data.zip\")\n",
    "\n",
    "if not os.path.exists(data_zip_path):\n",
    "    print(\"Download data\")\n",
    "    import urllib.request\n",
    "    u = urllib.request.URLopener()\n",
    "    u.retrieve(\"http://avalon.aut.bme.hu/~judit/resources/umbc/data.zip\", data_zip_path)\n",
    "    print(\"Data downloaded\")\n",
    "\n",
    "unzip_path = os.path.join(data_dir, \"umbc_sample_toy.txt\")\n",
    "\n",
    "if not os.path.exists(os.path.join(data_dir, \"umbc_sample_toy.txt\")) or \\\n",
    "        not os.path.exists(os.path.join(data_dir, \"umbc_sample_1M.txt\")):\n",
    "    print(\"Extracting data\")\n",
    "    from zipfile import ZipFile\n",
    "    with ZipFile(data_zip_path) as myzip:\n",
    "        myzip.extractall(data_dir)\n",
    "    print(\"Data extraction done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "301e56fa0e3fdb27dd1a913f04300aa8",
     "grade": false,
     "grade_id": "cell-70ef199bee1add72",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# 1. Write a generator function that reads a text corpus. (10 points)\n",
    "\n",
    "Each sentence occupies two lines. The first line is the sentence itself. Tokens (words) are separated by spaces.\n",
    "The second line is the POS (part-of-speech) tag of each token, also separated by space.\n",
    "Then comes an empty line before the next sentence.\n",
    "The end of the file may or may not contain an empty line.\n",
    "\n",
    "```\n",
    "This is a sentence .\n",
    "DT VBZ DT NN .\n",
    "\n",
    "This is the next sentence .\n",
    "DT VBZ DT JJ NN .\n",
    "```\n",
    "\n",
    "Your task is to create a generator function with one parameter, the file's name.\n",
    "The generator should yield one sentence at a time.\n",
    "\n",
    "A sentence is a list of token-POS tag pairs (tuples), so the first sentence in the example would look like this:\n",
    "\n",
    "```\n",
    "[('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('sentence', 'NN'), ('.', '.')]\n",
    "```\n",
    "\n",
    "and the second sentence:\n",
    "\n",
    "```\n",
    "[('This', 'DT'), ('is', 'VBZ'), ('the', 'DT'), ('next', 'JJ'), ('sentence', 'NN'), ('.', '.')]\n",
    "\n",
    "```\n",
    "Some sentences may be malformatted, and the number of tokens and POS tags differ, **skip** these sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "98001bcc0eaebab95b6948943858b51a",
     "grade": false,
     "grade_id": "cell-4d6620881903f657",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def read_corpus(fn):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "90e3352f4deada1f96c4ceea8916d05e",
     "grade": true,
     "grade_id": "cell-be0636a354bd3095",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "toy_data_fn = os.path.join(data_dir, \"umbc_sample_toy.txt\")\n",
    "\n",
    "toy_sentences = read_corpus(toy_data_fn)\n",
    "\n",
    "assert isinstance(toy_sentences, types.GeneratorType)\n",
    "\n",
    "toy_sentences = list(toy_sentences)\n",
    "\n",
    "# there are invalid sentences in the data, that you should skip\n",
    "assert len(toy_sentences) < 100\n",
    "assert toy_sentences[0] == [('Then', 'RB'),\n",
    " ('Jon', 'NNP'),\n",
    " ('falls', 'VBZ'),\n",
    " ('for', 'IN'),\n",
    " ('the', 'DT'),\n",
    " ('pretty', 'JJ'),\n",
    " ('new', 'JJ'),\n",
    " ('vet', 'NN'),\n",
    " ('played', 'VBN'),\n",
    " ('by', 'IN'),\n",
    " ('Jennifer', 'NNP'),\n",
    " ('Love', 'NNP'),\n",
    " ('Hewitt', 'NNP'),\n",
    " ('.', '.')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ffe0b6f2feaade661ade0acda219a430",
     "grade": false,
     "grade_id": "cell-74a7f4cfbd6c9f92",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# 2. Basic statistics (15 points)\n",
    "\n",
    "Compute basic statistics on the corpus. Try to iterate over the file only once and compute all of these statistics as you go.\n",
    "\n",
    "The statistics you are supposed to compute and store in the appropriate variables (see the tests below):\n",
    "\n",
    "1. number of sentences,\n",
    "2. number of tokens (words),\n",
    "3. number of types (unique words),\n",
    "4. length of the longest sentence,\n",
    "5. length of the shortest sentence,\n",
    "6. word frequencies,\n",
    "7. POS frequencies.\n",
    "\n",
    "You are **NOT** allowed to use `collections.Counter` but you are free to use `collections.defaultdict`.\n",
    "\n",
    "Use the variable `umbc_fn` as a parameter of `read_corpus`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "4a52e088d67b00b080949ec2bc464364",
     "grade": false,
     "grade_id": "cell-8ea5ae29dee1adee",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "umbc_fn = os.path.join(data_dir, \"umbc_sample_1M.txt\")\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a47ec648f97b74a5ef36c418242f6ad2",
     "grade": false,
     "grade_id": "cell-335896cfa3e4591b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"Number of sentences: {}\".format(sentence_no))\n",
    "print(\"Number of tokens (words): {}\".format(token_no))\n",
    "print(\"Number of types (unique words)): {}\".format(type_no))\n",
    "print(\"Length of the longest sentence: {}\".format(sentence_maxlen))\n",
    "print(\"Length of the shortest sentence: {}\".format(sentence_minlen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b0929a5aa0f305284e02695c7e9a8741",
     "grade": true,
     "grade_id": "cell-ea06292eacbe2c42",
     "locked": true,
     "points": 7,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(sentence_no) == int\n",
    "assert type(token_no) == int\n",
    "assert type(type_no) == int\n",
    "assert type(sentence_maxlen) == int\n",
    "assert type(sentence_minlen) == int\n",
    "assert sentence_no == 41989"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "293a9867c4d3f2303f33e27ef0856a5e",
     "grade": true,
     "grade_id": "cell-b6f744a5709a2432",
     "locked": true,
     "points": 3,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(word_freq) == defaultdict or type(word_freq) == dict\n",
    "assert type(pos_freq) == defaultdict or type(pos_freq) == dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8daa3cfd79544ab856e915f394410751",
     "grade": false,
     "grade_id": "cell-c5a14ee2664be3c0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 2.2 What are the 10 most frequent words and 5 most frequent POS tags.\n",
    "\n",
    "The lists should be in decreasing order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "83ece5544a32831c44a2c790856d833e",
     "grade": false,
     "grade_id": "cell-8f793d4194c41524",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "369d98141886e36f11bb514830533944",
     "grade": true,
     "grade_id": "cell-dc0e2c719dc647cf",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "print(most_frequent_words)\n",
    "print(most_frequent_pos)\n",
    "assert type(most_frequent_words) == list\n",
    "assert type(most_frequent_pos) == list\n",
    "assert len(most_frequent_words) == 10\n",
    "assert type(most_frequent_words[0]) == str\n",
    "assert most_frequent_pos[0] == 'NN'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "db5b99a91a43698fc06cf4ab029ba86a",
     "grade": false,
     "grade_id": "cell-2fcc8f5b2e06a1bd",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# 3. Hapax legomenon (10 points)\n",
    "\n",
    "Hapax legomenon or hapax is a word that only appears once in the dataset. Since the distribution of words roughly follows Zipf's distribution, regardless of the size of the corpus, there will always be a large number of hapax or words that only appear once.\n",
    "\n",
    "Aside from theoretical, there are technical reasons for filtering these words such as the fact that we have limited memory.\n",
    "\n",
    "Compute the number of hapax words in the corpus using your previous statistics.\n",
    "Do not read the file again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "c0183ff46ba8ecb935cc4c38a46bff12",
     "grade": false,
     "grade_id": "cell-2ab659ccf17f6757",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "fa96fa83e7fbfcd9fccd030ad304f63f",
     "grade": true,
     "grade_id": "cell-6ac7967eb6f14ea8",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "print(hapax_no)\n",
    "assert type(hapax_no) == int\n",
    "assert 0 < hapax_no < len(word_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2a1a8017140ce32dfd9600b2b0a03e6d",
     "grade": false,
     "grade_id": "cell-5b3dda2849c6204b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## The `_UNK_` symbol\n",
    "\n",
    "Rare words including hapax words are most often replaced by a common symbol such as `_UNK_` (unknown).\n",
    "During inference time (when running your model on unseen data), we may encounter words either never seen in the training data or deemed too rare and replaced with `_UNK_`.\n",
    "The best way to handle these words is to replace them with `_UNK_` since our model is prepared to handle these symbols.\n",
    "\n",
    "Follow this strategy in your viterbi function.\n",
    "\n",
    "Tip: you don't actually have to replace these words, you can check if they're in the `word_idx` dictionary. `dict.get` might be useful.\n",
    "\n",
    "## 3.1 Set of frequent words.\n",
    "\n",
    "Create a set of words that appear more at least `min_freq` times in the data.\n",
    "Do not read the file again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "74502be497732d073b9e416350023d97",
     "grade": false,
     "grade_id": "cell-2c844fe2043a74b4",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "min_freq = 2\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "cca47235e37ed907afa5ad2621ad1f58",
     "grade": true,
     "grade_id": "cell-970d67a9a7d8aea2",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "print(len(top_words))\n",
    "assert type(top_words) == set\n",
    "assert 0 < len(top_words) < len(word_freq)\n",
    "assert \"dog\" in top_words\n",
    "assert \"titillating\" not in top_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "05c9f1e9ef16bef826da60b7bbb73a9f",
     "grade": false,
     "grade_id": "cell-b71d2d3a5451784e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# 4. Mapping and counters (25 points)\n",
    "\n",
    "## 4.1 Word and POS mapping\n",
    "\n",
    "Create a word and a POS mapping that maps each word to its unique integer id. The word ids should range from 0 to `len(top_words)-1`, while the POS ids should range from 0 to `len(pos_idx)-1`.\n",
    "You only need to map the non-rare words.\n",
    "\n",
    "Include the unknown symbol (`_UNK_`) in the word mapping. What should its id be?\n",
    "\n",
    "Add a `START` symbol to the POS mapping. It will be used in the Viterbi algorithm.\n",
    "Do not read the file again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "4c46ea36d4f198ebfefb3e851ef84abc",
     "grade": false,
     "grade_id": "cell-95dfecb8a30f7124",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ee5307b939c4ce606a00ab7fa3e35468",
     "grade": true,
     "grade_id": "cell-5d34e89ea9a099df",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert len(word_idx) == len(top_words) + 1  # _UNK_ included\n",
    "assert '_UNK_' in word_idx\n",
    "assert 'START' in pos_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d1fa489c67ff4f4007436d286a1c6581",
     "grade": false,
     "grade_id": "cell-a89c3e0ad5a13921",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 4.2 Emission and transition counts\n",
    "\n",
    "\n",
    "Hidden Markov Models were introduced in the [Tagging lecture](https://github.com/bmeaut/python_nlp_2018_spring/blob/master/course_material/07_Tagging/07_Tagging_lecture.ipynb) as follows:\n",
    "\n",
    "\n",
    "* Like the Markov model, we take only the $n$ preceding tokens into consideration\n",
    "* The idea behind the model is very different:\n",
    "    * We imagine an automaton that is always in a **(hidden) state**\n",
    "    * In each state, it emits something we can observe\n",
    "    * The task is to find out which is _the most probable_ state sequence that generates the observations\n",
    "* In the POS tagging context,\n",
    "    * The words in the text are the **observed events**\n",
    "    * The POS tags are the hidden states\n",
    "    \n",
    "    \n",
    "You will build a simple Hidden Markov Model for English POS tagging.\n",
    "In this case $n$ equals 1, so only the previous token is taken into consideration.\n",
    "\n",
    "Transition and emission probabilities are derived from the corpus. Compute the emission and transition counts from the corpus. Do not iterate over the data more than once.\n",
    "\n",
    "You need to build two matrices as follows ($\\#$ means _count of_):\n",
    "\n",
    "* a $|L| \\times |V|$ matrix of integers\n",
    "  $$M(i,j) = \\# \\ i^\\text{th} \\text{ pos tag emitting word } j$$\n",
    "* an $|L|\\times |L|$ matrix of integers\n",
    "  $$N(i,j) = \\# \\ j^\\text{th} \\text{ pos after } i^\\text{th} \\text { pos}$$\n",
    "  \n",
    "**CAUTION** Both matrices are defined differently from the ones in the lab exercise.\n",
    "\n",
    "### How to handle the POS tag of the first word in a sentence?\n",
    "\n",
    "HMMs need an explicit way of handling starting probabilities which is usually solved with special start state(s).\n",
    "Build the transition matrix as if there is an artificial `START` state at the beginning of every sentence.\n",
    "You don't actually have to add a symbol to the sentence.\n",
    "The start state does not affect the emission probabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "aa0243ae2ca228f27048a8c0f70f20f2",
     "grade": false,
     "grade_id": "cell-db601053694db0b8",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0d79e569d598bcda1309c93047fffdb0",
     "grade": true,
     "grade_id": "cell-abf2adc1dbec1bd4",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "print(emission_counts.shape)\n",
    "print(transition_counts.shape)\n",
    "assert(emission_counts[pos_idx[\"NN\"], word_idx[\"_UNK_\"]] == 4717)\n",
    "\n",
    "dog_id = word_idx[\"dog\"]\n",
    "noun_id = pos_idx[\"NN\"]\n",
    "vbz_id = pos_idx[\"VBZ\"]\n",
    "noun_dog_count = emission_counts[noun_id, dog_id]\n",
    "print(\"The state NN emitted the word dog {} times\".format(emission_counts[noun_id, dog_id]))\n",
    "print(\"The state NN followed the state VBZ {} times\".format(transition_counts[noun_id, vbz_id]))\n",
    "assert noun_dog_count == 64\n",
    "\n",
    "assert type(emission_counts) == np.ndarray\n",
    "assert type(transition_counts) == np.ndarray\n",
    "\n",
    "# counts should not be negative\n",
    "assert np.all(emission_counts >= 0)\n",
    "assert np.all(transition_counts >= 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "89bb625f6a0232f2f9258ec742e89bf8",
     "grade": false,
     "grade_id": "cell-8558b5090e3cb8bd",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 4.3 Emission and transition probabilities\n",
    "\n",
    "Empirical emission and transition probabilities are defined as\n",
    "\n",
    "* a $|L|\\times |V|$ matrix of floats\n",
    "  $$P_1(i,j) = \\frac{\\# \\ i^\\text{th} \\text{ pos tag emitting word } j}{\\# \\ \\text{pos tag } i}$$\n",
    "* an $|L|\\times |L|$ matrix of floats\n",
    "  $$P_2(i,j) = \\frac{\\# \\ j^\\text{th} \\text{ pos after } i^\\text{th} \\text { pos}}{\\# \\ \\text{pos tag } i}$$\n",
    "\n",
    "Create these probability matrices using the transition and emission counts from the previous exercise.\n",
    "\n",
    "You should see a warning for emission probabilities since one of the rows sums to 0.\n",
    "It is the row corresponding to the `START` state.\n",
    "Add an artificial emission of the `_UNK_` symbol from the `START` state to solve this (see the tests below).\n",
    "\n",
    "Use 64-bit floats.\n",
    "Do not use log-probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "489830168102e0c707f8581f0ef84e8b",
     "grade": false,
     "grade_id": "cell-4b6fe7c5c4a718d2",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5814efc55eddb338ed203a93bea25270",
     "grade": true,
     "grade_id": "cell-3fd3068b4a7fad9e",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert np.count_nonzero(emission_counts[pos_idx['START']]) != 0\n",
    "assert type(emission_probs) == np.ndarray\n",
    "assert type(transition_probs) == np.ndarray\n",
    "assert emission_counts.shape == emission_probs.shape\n",
    "assert transition_counts.shape == transition_probs.shape\n",
    "assert np.allclose(emission_probs.sum(axis=1), 1)\n",
    "assert np.allclose(transition_probs.sum(axis=1), 1)\n",
    "\n",
    "# probabilities cannot be negative\n",
    "assert np.all(emission_probs >= 0)\n",
    "assert np.all(transition_probs >= 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a617bbef6d94a03a507ba3a9646b5377",
     "grade": false,
     "grade_id": "cell-92a83f6bfe1636d1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# 5. Viterbi algorithm (30 points)\n",
    "\n",
    "Implement the Viterbi algorithm for $n=1$ (the model only takes into account the previous state).\n",
    "\n",
    "The `viterbi` function takes five parameters:\n",
    "1. the sentence as a list of tokens,\n",
    "2. the transition probability matrix,\n",
    "3. the emission probability matrix,\n",
    "4. the word - id mapping, \n",
    "5. and the POS tag - id mapping,\n",
    "\n",
    "and it returns a list of POS tags.\n",
    "It is the same length as the input sentence and each POS tag corresponds to one token in the input sentence.\n",
    "\n",
    "Your function should handle out-of-vocabulary words as if they're `_UNK_` symbols.\n",
    "\n",
    "Remember that the HMM always starts from an artificial `START` stat, then proceeds to the first _real_ state (POS in this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "65de9f601fef0be9373219d179450b70",
     "grade": false,
     "grade_id": "cell-834f42f6a029e60b",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def viterbi(sentence, transitions, emissions, word_idx, pos_idx):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e14e1d2b1ab97978cd05ea62f111fd7a",
     "grade": true,
     "grade_id": "cell-729b37e27ef530af",
     "locked": true,
     "points": 30,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def run_viterbi(sentence):\n",
    "    \"\"\"Call viterbi with global parameters from previous exercises\"\"\"\n",
    "    if isinstance(sentence, str):\n",
    "        sentence = sentence.split()\n",
    "    return viterbi(sentence, transition_probs, emission_probs, word_idx, pos_idx)\n",
    "\n",
    "tags = run_viterbi(\"The cat runs .\")\n",
    "print(tags)\n",
    "assert tags == ['DT', 'NN', 'VBZ', '.']\n",
    "work1 = run_viterbi(\"My work here is done .\")\n",
    "work2= run_viterbi(\"I work here .\")\n",
    "print(\"Work should receive different POS tags\\n\"\n",
    "      \"My work here is done: {}\\n\"\n",
    "      \"I work here: {}\".format(work1[1], work2[1]))\n",
    "print(viterbi([\"The\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"fox\", \".\"],\n",
    "              transition_probs, emission_probs, word_idx, pos_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c5e83cd31edc4db1bbef0ad42647e3b9",
     "grade": false,
     "grade_id": "cell-8cde8063b129a6d6",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 5.2 Addtional tests (extra 10 points)\n",
    "\n",
    "Add your own tests that demonstrate the effect of context in POS tagging such as the previous example, where _work_ has a different POS tag in different context.\n",
    "\n",
    "2 points / example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "5320fd0e73a15865cc5ea4d21c126a78",
     "grade": true,
     "grade_id": "cell-7723e811eb219319",
     "locked": false,
     "points": 10,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1f4d69f8d265645def9cb379e4706edf",
     "grade": false,
     "grade_id": "cell-9672fc9c7981f51d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# PEP8, Code cleanness (10 points)\n",
    "\n",
    "This cell is here for technical reasons, you will receive feedback on your code quality here. You do not need to write anything here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "edb95118c918b3d8f6660b7c6f363d80",
     "grade": true,
     "grade_id": "cell-075b12bb3e4c677f",
     "locked": false,
     "points": 10,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
